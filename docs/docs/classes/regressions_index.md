---
layout: default
title: regressions
parent: Dataspot
grand_parent: Dataspot
has_children: true
---

# regressions

In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships
between a dependent variable (often called the 'outcome variable') and one or more independent variables
(often called 'predictors', 'covariates', or 'features'). The most common form of regression analysis is linear
regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits
the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes
the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line
(or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate
the conditional expectation (or population average value) of the dependent variable when the independent variables
take on a given set of values. Less common forms of regression use slightly different procedures to estimate
alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the
conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).
